{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Kode Lengkap - Fase 4.3 (Retry): Tuning XGBoost dengan Early Stopping Langsung\n",
    "\n",
    "Asumsi:\n",
    "- Library (xgboost 3.0.0+, optuna, pandas, numpy, scikit-learn) terinstal.\n",
    "- Kernel sudah di-restart.\n",
    "- Variabel X, y, X_test, N_SPLITS, RANDOM_SEED,\n",
    "  submission_id_col, submission_target_col, test_ids sudah didefinisikan.\n",
    "- Variabel best_trial (dari LGBM) ada untuk perbandingan (opsional).\n",
    "\"\"\"\n",
    "\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning) # Mengabaikan beberapa user warning Optuna/XGBoost\n",
    "\n",
    "print(\"\\n--- Fase 4.3 (Retry): Hyperparameter Tuning XGBoost ---\")\n",
    "print(f\"Menggunakan XGBoost versi: {xgb.__version__}\") # Verifikasi lagi\n",
    "\n",
    "# --- Memastikan data X dan y siap ---\n",
    "if 'X' not in locals() or 'y' not in locals():\n",
    "    print(\"ERROR: Variabel X atau y belum didefinisikan. Harap jalankan sel persiapan data sebelumnya.\")\n",
    "    # Muat ulang atau pastikan variabel ada\n",
    "    try:\n",
    "        X = train_final.copy()\n",
    "        y = y_train.copy()\n",
    "        X_test = test_final.copy()\n",
    "        non_numeric_cols = X.select_dtypes(exclude=np.number).columns\n",
    "        if not non_numeric_cols.empty:\n",
    "            X = X.drop(columns=non_numeric_cols)\n",
    "            cols_to_drop_in_test = [col for col in non_numeric_cols if col in X_test.columns]\n",
    "            if cols_to_drop_in_test: X_test = X_test.drop(columns=cols_to_drop_in_test)\n",
    "        print(\"Data X dan y siap.\")\n",
    "    except NameError:\n",
    "        print(\"Gagal memuat ulang X/y. Hentikan.\")\n",
    "        exit()\n",
    "elif X.select_dtypes(exclude=np.number).shape[1] > 0:\n",
    "    print(\"ERROR: Masih ada kolom non-numerik di X. Membersihkan...\")\n",
    "    non_numeric_cols = X.select_dtypes(exclude=np.number).columns\n",
    "    X = X.drop(columns=non_numeric_cols)\n",
    "    cols_to_drop_in_test = [col for col in non_numeric_cols if col in X_test.columns]\n",
    "    if cols_to_drop_in_test: X_test = X_test.drop(columns=cols_to_drop_in_test)\n",
    "    print(\"Kolom non-numerik dihapus.\")\n",
    "\n",
    "\n",
    "# --- Fungsi Objective untuk Optuna (Menggunakan early_stopping_rounds langsung) ---\n",
    "def objective_xgb_direct(trial):\n",
    "    # Definisikan ruang pencarian hyperparameter\n",
    "    xgb_params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 100),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 10.0, log=True),\n",
    "        'seed': RANDOM_SEED,\n",
    "        'nthread': -1,\n",
    "        'tree_method': 'hist'\n",
    "    }\n",
    "    n_estimators_xgb = trial.suggest_int('n_estimators', 500, 3000, step=100)\n",
    "    early_stopping_rounds_xgb = 50\n",
    "\n",
    "    # Setup K-Fold\n",
    "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "    fold_scores = []\n",
    "\n",
    "    # Loop Cross-Validation\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train_fold, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        # Inisialisasi model XGBoost\n",
    "        model = xgb.XGBRegressor(**xgb_params, n_estimators=n_estimators_xgb)\n",
    "\n",
    "        # Latih model dengan early stopping langsung\n",
    "        try:\n",
    "            model.fit(X_train, y_train_fold,\n",
    "                      eval_set=[(X_val, y_val)], # Data validasi\n",
    "                      early_stopping_rounds=early_stopping_rounds_xgb, # Argumen langsung\n",
    "                      verbose=False)             # Matikan log training\n",
    "        except TypeError as e:\n",
    "            # Ini seharusnya tidak terjadi dengan XGBoost v3\n",
    "            print(f\"*** TERJADI TypeError saat fit di fold {fold+1}: {e} ***\")\n",
    "            print(\"*** Ini sangat aneh. Periksa instalasi XGBoost Anda. ***\")\n",
    "            raise e # Hentikan trial jika error\n",
    "        except Exception as e:\n",
    "             print(f\"Error tidak terduga saat fit di fold {fold+1}: {e}\")\n",
    "             raise e\n",
    "\n",
    "        # Buat prediksi & hitung skor\n",
    "        val_preds = model.predict(X_val)\n",
    "        fold_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "        fold_scores.append(fold_rmse)\n",
    "\n",
    "        # Pruning Optuna\n",
    "        trial.report(fold_rmse, fold)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    # Kembalikan rata-rata RMSE\n",
    "    return np.mean(fold_scores)\n",
    "\n",
    "# --- Konfigurasi dan Eksekusi Studi Optuna ---\n",
    "N_TRIALS_XGB = 50 # Jumlah trial\n",
    "# Gunakan nama studi baru untuk memastikan tidak ada konflik state\n",
    "STUDY_NAME_XGB = 'xgb_hydrostatic_tuning_direct_v4'\n",
    "DB_FILENAME = 'optuna_studies.db'\n",
    "\n",
    "# Membuat atau Memuat Studi\n",
    "try:\n",
    "    study_xgb = optuna.load_study(study_name=STUDY_NAME_XGB, storage=f'sqlite:///{DB_FILENAME}')\n",
    "    print(f\"Memuat studi Optuna XGBoost '{STUDY_NAME_XGB}' dari {DB_FILENAME}\")\n",
    "    remaining_trials = N_TRIALS_XGB - len(study_xgb.trials)\n",
    "    if remaining_trials <= 0: N_TRIALS_TO_RUN = 0; print(\"Jumlah trials sudah tercapai.\")\n",
    "    else: N_TRIALS_TO_RUN = remaining_trials; print(f\"Akan menjalankan {remaining_trials} trial tambahan.\")\n",
    "except KeyError:\n",
    "    print(f\"Membuat studi Optuna XGBoost baru '{STUDY_NAME_XGB}' di {DB_FILENAME}\")\n",
    "    study_xgb = optuna.create_study(direction='minimize',\n",
    "                                    study_name=STUDY_NAME_XGB,\n",
    "                                    storage=f'sqlite:///{DB_FILENAME}',\n",
    "                                    load_if_exists=True)\n",
    "    N_TRIALS_TO_RUN = N_TRIALS_XGB\n",
    "\n",
    "# Menjalankan Optimasi\n",
    "if N_TRIALS_TO_RUN > 0:\n",
    "    print(f\"\\nMemulai optimasi XGBoost dengan {N_TRIALS_TO_RUN} trials...\")\n",
    "    start_opt_time = time.time()\n",
    "    study_xgb.optimize(objective_xgb_direct, n_trials=N_TRIALS_TO_RUN, timeout=600) # Timeout opsional\n",
    "    end_opt_time = time.time()\n",
    "    print(f\"Optimasi selesai dalam {end_opt_time - start_opt_time:.2f} detik.\")\n",
    "else:\n",
    "    print(\"\\nTidak ada trial baru yang dijalankan.\")\n",
    "\n",
    "# --- Menampilkan Hasil Terbaik ---\n",
    "print(\"\\n--- Hasil Tuning Optuna XGBoost ---\")\n",
    "if len(study_xgb.trials) > 0:\n",
    "    completed_trials = [t for t in study_xgb.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "    if completed_trials:\n",
    "        print(f\"Jumlah trials selesai total: {len(study_xgb.trials)}\")\n",
    "        print(f\"Jumlah trials COMPLETE: {len(completed_trials)}\")\n",
    "        best_trial_xgb = study_xgb.best_trial\n",
    "        print(f\"Trial XGBoost terbaik:\")\n",
    "        print(f\"  Value (Min RMSE): {best_trial_xgb.value:.6f}\")\n",
    "        print(f\"  Params: \")\n",
    "        for key, value in best_trial_xgb.params.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "\n",
    "        best_xgb_params = { # Simpan parameter terbaik\n",
    "            'objective': 'reg:squarederror', 'eval_metric': 'rmse',\n",
    "            'seed': RANDOM_SEED, 'nthread': -1, 'tree_method': 'hist'\n",
    "        }\n",
    "        best_xgb_params.update(best_trial_xgb.params)\n",
    "        print(\"\\nParameter XGBoost terbaik ditemukan ('best_xgb_params').\")\n",
    "\n",
    "        # --- Perbandingan dengan LGBM ---\n",
    "        try:\n",
    "            best_lgbm_rmse = best_trial.value\n",
    "            best_xgb_rmse = best_trial_xgb.value\n",
    "            print(f\"\\nPerbandingan Skor CV Terbaik:\")\n",
    "            print(f\"LGBM: {best_lgbm_rmse:.6f}\")\n",
    "            print(f\"XGBoost: {best_xgb_rmse:.6f}\")\n",
    "            proceed_to_ensemble = (best_xgb_rmse <= best_lgbm_rmse + 0.00001)\n",
    "            if proceed_to_ensemble: print(\"\\nXGBoost kompetitif atau lebih baik!\")\n",
    "            else: print(\"\\nXGBoost tidak lebih baik dari LGBM.\")\n",
    "        except NameError:\n",
    "            print(\"\\nVariabel 'best_trial' (LGBM) tidak ditemukan untuk perbandingan.\")\n",
    "            proceed_to_ensemble = True # Asumsikan lanjut jika hanya tuning XGB\n",
    "        except Exception as e:\n",
    "            print(f\"Error saat membandingkan skor: {e}\")\n",
    "            proceed_to_ensemble = True\n",
    "    else:\n",
    "        print(\"\\nTidak ada trial Optuna yang berhasil diselesaikan (COMPLETE).\")\n",
    "        best_xgb_params = None\n",
    "        proceed_to_ensemble = False\n",
    "else:\n",
    "    print(\"\\nTidak ada trial Optuna yang pernah dijalankan untuk studi ini.\")\n",
    "    best_xgb_params = None\n",
    "    proceed_to_ensemble = False\n",
    "\n",
    "# --- Kode Selanjutnya (Ensemble/Submission) ---\n",
    "# (Tambahkan kode ensemble atau submission di sini jika proceed_to_ensemble=True dan best_xgb_params ada)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if proceed_to_ensemble:\n",
    "    print(\"\\n--- Fase 4.4: Menghasilkan Prediksi OOF & Test untuk Ensemble ---\")\n",
    "\n",
    "    # Gunakan parameter terbaik untuk kedua model\n",
    "    final_lgbm_params = best_lgbm_params.copy() # Dari tuning LGBM sebelumnya\n",
    "    final_xgb_params = best_xgb_params.copy()   # Dari tuning XGB baru saja\n",
    "\n",
    "    # Reset/Inisialisasi array penyimpanan\n",
    "    oof_lgbm = np.zeros(X.shape[0])\n",
    "    oof_xgb = np.zeros(X.shape[0])\n",
    "    test_lgbm = np.zeros(X_test.shape[0]) if not X_test.empty else np.array([])\n",
    "    test_xgb = np.zeros(X_test.shape[0]) if not X_test.empty else np.array([])\n",
    "    fold_scores_lgbm_ens = []\n",
    "    fold_scores_xgb_ens = []\n",
    "\n",
    "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "    early_stopping_rounds_lgbm = 50\n",
    "    early_stopping_rounds_xgb = 50\n",
    "\n",
    "    print(f\"Menjalankan {N_SPLITS}-Fold CV untuk kedua model...\")\n",
    "    start_time_ens = time.time()\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n",
    "        print(f\"--- Fold {fold+1}/{N_SPLITS} ---\")\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train_fold, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        # --- Train LGBM ---\n",
    "        model_lgbm = lgb.LGBMRegressor(**final_lgbm_params)\n",
    "        model_lgbm.fit(X_train, y_train_fold,\n",
    "                       eval_set=[(X_val, y_val)],\n",
    "                       eval_metric='rmse',\n",
    "                       callbacks=[lgb.early_stopping(stopping_rounds=early_stopping_rounds_lgbm, verbose=False)])\n",
    "        oof_lgbm[val_index] = model_lgbm.predict(X_val)\n",
    "        fold_scores_lgbm_ens.append(np.sqrt(mean_squared_error(y_val, oof_lgbm[val_index])))\n",
    "        if not X_test.empty:\n",
    "            test_lgbm += model_lgbm.predict(X_test) / N_SPLITS\n",
    "        print(f\"  LGBM Fold RMSE: {fold_scores_lgbm_ens[-1]:.6f}\")\n",
    "\n",
    "        # --- Train XGBoost ---\n",
    "        model_xgb = xgb.XGBRegressor(**final_xgb_params) # n_estimators sudah ada di dalam dict\n",
    "        model_xgb.fit(X_train, y_train_fold,\n",
    "                      eval_set=[(X_val, y_val)],\n",
    "                      early_stopping_rounds=early_stopping_rounds_xgb,\n",
    "                      verbose=False)\n",
    "        oof_xgb[val_index] = model_xgb.predict(X_val)\n",
    "        fold_scores_xgb_ens.append(np.sqrt(mean_squared_error(y_val, oof_xgb[val_index])))\n",
    "        if not X_test.empty:\n",
    "            test_xgb += model_xgb.predict(X_test) / N_SPLITS\n",
    "        print(f\"  XGBoost Fold RMSE: {fold_scores_xgb_ens[-1]:.6f}\")\n",
    "\n",
    "\n",
    "    total_time_ens = time.time() - start_time_ens\n",
    "    print(f\"\\nPelatihan ensemble selesai dalam {total_time_ens:.2f} detik.\")\n",
    "\n",
    "    # --- Evaluasi Ensemble (Simple Average) ---\n",
    "    print(\"\\n--- Evaluasi Ensemble (Simple Average) ---\")\n",
    "    oof_ensemble = (oof_lgbm + oof_xgb) / 2\n",
    "    rmse_oof_lgbm = np.sqrt(mean_squared_error(y, oof_lgbm))\n",
    "    rmse_oof_xgb = np.sqrt(mean_squared_error(y, oof_xgb))\n",
    "    rmse_oof_ensemble = np.sqrt(mean_squared_error(y, oof_ensemble))\n",
    "\n",
    "    print(f\"RMSE OOF LGBM:     {rmse_oof_lgbm:.6f}\")\n",
    "    print(f\"RMSE OOF XGBoost:  {rmse_oof_xgb:.6f}\")\n",
    "    print(f\"RMSE OOF Ensemble: {rmse_oof_ensemble:.6f}\")\n",
    "\n",
    "    # Keputusan untuk Submission Ensemble\n",
    "    if rmse_oof_ensemble < min(rmse_oof_lgbm, rmse_oof_xgb):\n",
    "        print(\"\\nEnsemble memberikan peningkatan pada skor OOF! Membuat submission ensemble.\")\n",
    "        # Buat prediksi test ensemble\n",
    "        test_ensemble = (test_lgbm + test_xgb) / 2\n",
    "        # Pastikan tidak ada prediksi negatif (jika relevan)\n",
    "        # test_ensemble[test_ensemble < 0] = 0\n",
    "\n",
    "        # Buat DataFrame submission ensemble\n",
    "        submission_ensemble_df = pd.DataFrame({\n",
    "            submission_id_col: test_ids,\n",
    "            submission_target_col: test_ensemble\n",
    "        })\n",
    "\n",
    "        # Simpan ke file CSV\n",
    "        submission_ensemble_filename = 'submission_ensemble_lgbm_xgb.csv'\n",
    "        submission_ensemble_df.to_csv(submission_ensemble_filename, index=False)\n",
    "        print(f\"\\nFile submission ensemble '{submission_ensemble_filename}' telah dibuat.\")\n",
    "        print(submission_ensemble_df.head())\n",
    "\n",
    "    else:\n",
    "        print(\"\\nEnsemble tidak memberikan peningkatan signifikan pada skor OOF.\")\n",
    "        print(\"Sebaiknya gunakan model tunggal terbaik (LGBM atau XGBoost) untuk submission.\")\n",
    "        # Anda bisa membuat submission dari model terbaik tunggal jika mau\n",
    "        # (misal, gunakan kode dari Opsi 1 sebelumnya dengan model & prediksi yang sesuai)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
